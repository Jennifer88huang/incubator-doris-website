

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>BROKER LOAD &mdash; Doris Documentations 0.11.0 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../../../../../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../../../" src="../../../../../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../../../../../_static/jquery.js"></script>
        <script type="text/javascript" src="../../../../../_static/underscore.js"></script>
        <script type="text/javascript" src="../../../../../_static/doctools.js"></script>
        <script type="text/javascript" src="../../../../../_static/language_data.js"></script>
    
    <script type="text/javascript" src="../../../../../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../search.html" />
    <link rel="next" title="CANCEL DELETE" href="CANCEL DELETE_EN.html" />
    <link rel="prev" title="Data Manipulation" href="index.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../../../../index.html" class="icon icon-home"> Doris Documentations
          

          
          </a>

          
            
            
              <div class="version">
                0.11.0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../../../cn/index.html">中文</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../../../index.html">English</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../../../installing/index.html">Compilation and deployment</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../getting-started/index.html">Getting started</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../administrator-guide/index.html">Administrator Guide</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../extending-doris/index.html">Extending Ability</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../internal/index.html">Design Documents</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../../index.html">SQL Manual</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="../../sql-functions/index.html">SQL Functions</a></li>
<li class="toctree-l3 current"><a class="reference internal" href="../index.html">DDL Statements</a><ul class="current">
<li class="toctree-l4"><a class="reference internal" href="../Account Management/index.html">Account Management</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Administration/index.html">Administration</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Data Definition/index.html">Data Definition</a></li>
<li class="toctree-l4 current"><a class="reference internal" href="index.html">Data Manipulation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Data Types/index.html">Data Types</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Utility/index.html">Utility</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../community/index.html">Apache Commnity</a></li>
</ul>
</li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../../index.html">Doris Documentations</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../../../index.html">English</a> &raquo;</li>
        
          <li><a href="../../index.html">SQL Manual</a> &raquo;</li>
        
          <li><a href="../index.html">DDL Statements</a> &raquo;</li>
        
          <li><a href="index.html">Data Manipulation</a> &raquo;</li>
        
      <li>BROKER LOAD</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            
              <a href="https://github.com/apache/incubator-doris/blob/master/docs/documentation/en/sql-reference/sql-statements/Data Manipulation/BROKER LOAD_EN.md" class="fa fa-github"> Edit on GitHub</a>
            
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="broker-load">
<h1>BROKER LOAD<a class="headerlink" href="#broker-load" title="Permalink to this headline">¶</a></h1>
<div class="section" id="description">
<h2>description<a class="headerlink" href="#description" title="Permalink to this headline">¶</a></h2>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Broker load will load data into Doris via Broker.
Use `show broker;` to see the Broker deployed in cluster.

Support following data sources:

1. Baidu HDFS: hdfs for Baidu. Only be used inside Baidu.
2. Baidu AFS: afs for Baidu. Only be used inside Baidu.
3. Baidu Object Storage(BOS): BOS on Baidu Cloud.
4. Apache HDFS.
</pre></div>
</div>
<div class="section" id="syntax">
<h3>Syntax:<a class="headerlink" href="#syntax" title="Permalink to this headline">¶</a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>LOAD LABEL load_label
(
data_desc1[, data_desc2, ...]
)
WITH BROKER broker_name
[broker_properties]
[opt_properties];

1. load_label

    Unique load label within a database.
    syntax: 
    [database_name.]your_label
 
2. data_desc

    To describe the data source. 
    syntax: 
        DATA INFILE
        (
        &quot;file_path1&quot;[, file_path2, ...]
        )
        [NEGATIVE]
        INTO TABLE `table_name`
        [PARTITION (p1, p2)]
        [COLUMNS TERMINATED BY &quot;column_separator&quot;]
        [FORMAT AS &quot;file_type&quot;]
        [(column_list)]
        [SET (k1 = func(k2))]
        [WHERE predicate]    

    Explain: 
        file_path: 

        File path. Support wildcard. Must match to file, not directory. 

        PARTITION:

        Data will only be loaded to specified partitions. Data out of partition&#39;s range will be filtered. If not specifed, all partitions will be loaded.
                
        NEGATIVE: 
        
        If this parameter is specified, it is equivalent to importing a batch of &quot;negative&quot; data to offset the same batch of data loaded before.
        
        This parameter applies only to the case where there are value columns and the aggregation type of value columns is only SUM.
        
        column_separator: 
        
        Used to specify the column separator in the import file. Default is `\t`.
        If the character is invisible, it needs to be prefixed with `\\x`, using hexadecimal to represent the separator.

        For example, the separator `\x01` of the hive file is specified as `\\ x01`
        
        file_type: 

        Used to specify the type of imported file, such as parquet, csv. Default values are determined by the file suffix name. 

        column_list: 

        Used to specify the correspondence between columns in the import file and columns in the table.

        When you need to skip a column in the import file, specify it as a column name that does not exist in the table.

        syntax: 
        (col_name1, col_name2, ...)
        
        SET:
        
        If this parameter is specified, a column of the source file can be transformed according to a function, and then the transformed result can be loaded into the table. The grammar is `column_name = expression`. Some examples are given to help understand.

        Example 1: There are three columns &quot;c1, c2, c3&quot; in the table. The first two columns in the source file correspond in turn (c1, c2), and the last two columns correspond to c3. Then, column (c1, c2, tmp_c3, tmp_c4) SET (c3 = tmp_c3 + tmp_c4) should be specified.

        Example 2: There are three columns &quot;year, month, day&quot; in the table. There is only one time column in the source file, in the format of &quot;2018-06-01:02:03&quot;. Then you can specify columns (tmp_time) set (year = year (tmp_time), month = month (tmp_time), day = day (tmp_time)) to complete the import.

        WHERE:
      
        After filtering the transformed data, data that meets where predicates can be loaded. Only column names in tables can be referenced in WHERE statements.
        
3. broker_name

    The name of the Broker used can be viewed through the `show broker` command.

4. broker_properties

    Used to provide Broker access to data sources. Different brokers, and different access methods, need to provide different information.

    1. Baidu HDFS/AFS

        Access to Baidu&#39;s internal hdfs/afs currently only supports simple authentication, which needs to be provided:
        
        username: hdfs username
        password: hdfs password

    2. BOS

        bos_endpoint.
        bos_accesskey: cloud user&#39;s accesskey
        bos_secret_accesskey: cloud user&#39;s secret_accesskey
    
    3. Apache HDFS

        Community version of HDFS supports simple authentication, Kerberos authentication, and HA configuration.

        Simple authentication:
        hadoop.security.authentication = simple (default)
        username: hdfs username
        password: hdfs password

        kerberos authentication: 
        hadoop.security.authentication = kerberos
        kerberos_principal:  kerberos&#39;s principal
        kerberos_keytab:  path of kerberos&#39;s keytab file. This file should be able to access by Broker
        kerberos_keytab_content: Specify the contents of the KeyTab file in Kerberos after base64 encoding. This option is optional from the kerberos_keytab configuration. 

        namenode HA: 
        By configuring namenode HA, new namenode can be automatically identified when the namenode is switched
        dfs.nameservices: hdfs service name，customize，eg: &quot;dfs.nameservices&quot; = &quot;my_ha&quot;
        dfs.ha.namenodes.xxx: Customize the name of a namenode, separated by commas. XXX is a custom name in dfs. name services, such as &quot;dfs. ha. namenodes. my_ha&quot; = &quot;my_nn&quot;
        dfs.namenode.rpc-address.xxx.nn: Specify RPC address information for namenode, where NN denotes the name of the namenode configured in dfs.ha.namenodes.xxxx, such as: &quot;dfs.namenode.rpc-address.my_ha.my_nn&quot;= &quot;host:port&quot;
        dfs.client.failover.proxy.provider: Specify the provider that client connects to namenode by default: org. apache. hadoop. hdfs. server. namenode. ha. Configured Failover ProxyProvider.

4. opt_properties

    Used to specify some special parameters. 
    Syntax: 
    [PROPERTIES (&quot;key&quot;=&quot;value&quot;, ...)]
    
    You can specify the following parameters: 
    
    timout: Specifies the timeout time for the import operation. The default timeout is 4 hours per second.

    max_filter_ratio: Data ratio of maximum tolerance filterable (data irregularity, etc.). Default zero tolerance.

    exc_mem_limit: Sets the upper memory limit for import. The default is 2G, per byte. This is the upper memory limit for a single BE node.

    A load may be distributed over multiple BEs. Let&#39;s assume that 1GB data needs up to 5GB of memory for processing at a single node. Suppose 1GB files are distributed over two nodes, then theoretically, each node needs 2.5GB of memory. Then the parameter can be set to 268454560, or 2.5GB.

    strict_mode: Whether the data is strictly restricted. The default is true.

    timezone: Specify time zones for functions affected by time zones, such as strftime/alignment_timestamp/from_unixtime, etc. See the documentation for details. If not specified, use the &quot;Asia/Shanghai&quot; time zone.

5. Load data format sample

    Integer（TINYINT/SMALLINT/INT/BIGINT/LARGEINT）: 1, 1000, 1234
    Float（FLOAT/DOUBLE/DECIMAL）: 1.1, 0.23, .356
    Date（DATE/DATETIME）: 2017-10-03, 2017-06-13 12:34:03. 
    (Note: If it&#39;s in other date formats, you can use strftime or time_format functions to convert in the import command)
    
    String（CHAR/VARCHAR）: &quot;I am a student&quot;, &quot;a&quot;
    NULL: \N
</pre></div>
</div>
</div>
</div>
<div class="section" id="example">
<h2>example<a class="headerlink" href="#example" title="Permalink to this headline">¶</a></h2>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>1. Load a batch of data from HDFS, specify timeout and filtering ratio. Use the broker with the inscription my_hdfs_broker. Simple authentication.

    LOAD LABEL example_db.label1
    (
    DATA INFILE(&quot;hdfs://hdfs_host:hdfs_port/user/palo/data/input/file&quot;)
    INTO TABLE `my_table`
    )
    WITH BROKER my_hdfs_broker
    (
    &quot;username&quot; = &quot;hdfs_user&quot;,
    &quot;password&quot; = &quot;hdfs_passwd&quot;
    )
    PROPERTIES
    (
    &quot;timeout&quot; = &quot;3600&quot;,
    &quot;max_filter_ratio&quot; = &quot;0.1&quot;
    );

    Where hdfs_host is the host of the namenode and hdfs_port is the fs.defaultFS port (default 9000)
    
2. Load a batch of data from AFS contains multiple files. Import different tables, specify separators, and specify column correspondences.

    LOAD LABEL example_db.label2
    (
    DATA INFILE(&quot;afs://afs_host:hdfs_port/user/palo/data/input/file1&quot;)
    INTO TABLE `my_table_1`
    COLUMNS TERMINATED BY &quot;,&quot;
    (k1, k3, k2, v1, v2),
    DATA INFILE(&quot;afs://afs_host:hdfs_port/user/palo/data/input/file2&quot;)
    INTO TABLE `my_table_2`
    COLUMNS TERMINATED BY &quot;\t&quot;
    (k1, k2, k3, v2, v1)
    )
    WITH BROKER my_afs_broker
    (
    &quot;username&quot; = &quot;afs_user&quot;,
    &quot;password&quot; = &quot;afs_passwd&quot;
    )
    PROPERTIES
    (
    &quot;timeout&quot; = &quot;3600&quot;,
    &quot;max_filter_ratio&quot; = &quot;0.1&quot;
    );
    

3. Load a batch of data from HDFS, specify hive&#39;s default delimiter \\x01, and use wildcard * to specify all files in the directory. Use simple authentication and configure namenode HA at the same time

    LOAD LABEL example_db.label3
    (
    DATA INFILE(&quot;hdfs://hdfs_host:hdfs_port/user/palo/data/input/*&quot;)
    INTO TABLE `my_table`
    COLUMNS TERMINATED BY &quot;\\x01&quot;
    )
    WITH BROKER my_hdfs_broker
    (
    &quot;username&quot; = &quot;hdfs_user&quot;,
    &quot;password&quot; = &quot;hdfs_passwd&quot;,
    &quot;dfs.nameservices&quot; = &quot;my_ha&quot;,
    &quot;dfs.ha.namenodes.my_ha&quot; = &quot;my_namenode1, my_namenode2&quot;,
    &quot;dfs.namenode.rpc-address.my_ha.my_namenode1&quot; = &quot;nn1_host:rpc_port&quot;,
    &quot;dfs.namenode.rpc-address.my_ha.my_namenode2&quot; = &quot;nn2_host:rpc_port&quot;,
    &quot;dfs.client.failover.proxy.provider&quot; = &quot;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&quot;
    )

4. Load a batch of &quot;negative&quot; data from HDFS. Use Kerberos authentication to provide KeyTab file path.

    LOAD LABEL example_db.label4
    (
    DATA INFILE(&quot;hdfs://hdfs_host:hdfs_port/user/palo/data/input/old_file)
    NEGATIVE
    INTO TABLE `my_table`
    COLUMNS TERMINATED BY &quot;\t&quot;
    )
    WITH BROKER my_hdfs_broker
    (
    &quot;hadoop.security.authentication&quot; = &quot;kerberos&quot;,
    &quot;kerberos_principal&quot;=&quot;doris@YOUR.COM&quot;,
    &quot;kerberos_keytab&quot;=&quot;/home/palo/palo.keytab&quot;
    )

5. Load a batch of data from HDFS, specify partition. At the same time, use Kerberos authentication mode. Provide the KeyTab file content encoded by base64.

    LOAD LABEL example_db.label5
    (
    DATA INFILE(&quot;hdfs://hdfs_host:hdfs_port/user/palo/data/input/file&quot;)
    INTO TABLE `my_table`
    PARTITION (p1, p2)
    COLUMNS TERMINATED BY &quot;,&quot;
    (k1, k3, k2, v1, v2)
    )
    WITH BROKER my_hdfs_broker
    (
    &quot;hadoop.security.authentication&quot;=&quot;kerberos&quot;,
    &quot;kerberos_principal&quot;=&quot;doris@YOUR.COM&quot;,
    &quot;kerberos_keytab_content&quot;=&quot;BQIAAABEAAEACUJBSURVLkNPTQAEcGFsbw&quot;
    )

6. Load a batch of data from BOS, specify partitions, and make some transformations to the columns of the imported files, as follows:

   Table schema: 
    k1 varchar(20)
    k2 int

    Assuming that the data file has only one row of data:

    Adele,1,1

    The columns in the data file correspond to the columns specified in the load statement:
    
    k1,tmp_k2,tmp_k3

    transform as: 

    1) k1: unchanged
    2) k2: sum of tmp_k2 and tmp_k3

    LOAD LABEL example_db.label6
    (
    DATA INFILE(&quot;bos://my_bucket/input/file&quot;)
    INTO TABLE `my_table`
    PARTITION (p1, p2)
    COLUMNS TERMINATED BY &quot;,&quot;
    (k1, tmp_k2, tmp_k3)
    SET (
      k2 = tmp_k2 + tmp_k3
    )
    )
    WITH BROKER my_bos_broker
    (
    &quot;bos_endpoint&quot; = &quot;http://bj.bcebos.com&quot;,
    &quot;bos_accesskey&quot; = &quot;xxxxxxxxxxxxxxxxxxxxxxxxxx&quot;,
    &quot;bos_secret_accesskey&quot;=&quot;yyyyyyyyyyyyyyyyyyyy&quot;
    )

7. Load data into tables containing HLL columns, which can be columns in tables or columns in data

    If there are three columns in the table (id, v1, v2). The V1 and V2 columns are HLL columns. The imported source file has three columns. Then (column_list) declares that the first column is id, and the second and third columns are temporarily named k1, k2.

    In SET, the HLL column in the table must be specifically declared hll_hash. The V1 column in the table is equal to the hll_hash (k1) column in the original data.        

    LOAD LABEL example_db.label7
    (
    DATA INFILE(&quot;hdfs://hdfs_host:hdfs_port/user/palo/data/input/file&quot;)
    INTO TABLE `my_table`
    PARTITION (p1, p2)
    COLUMNS TERMINATED BY &quot;,&quot;
    (id, k1, k2)
    SET (
      v1 = hll_hash(k1),
      v2 = hll_hash(k2)
    )
    )
    WITH BROKER hdfs (&quot;username&quot;=&quot;hdfs_user&quot;, &quot;password&quot;=&quot;hdfs_password&quot;);

    LOAD LABEL example_db.label8
    (
    DATA INFILE(&quot;hdfs://hdfs_host:hdfs_port/user/palo/data/input/file&quot;)
    INTO TABLE `my_table`
    PARTITION (p1, p2)
    COLUMNS TERMINATED BY &quot;,&quot;
    (k1, k2, tmp_k3, tmp_k4, v1, v2)
    SET (
      v1 = hll_hash(tmp_k3),
      v2 = hll_hash(tmp_k4)
    )
    )
    WITH BROKER hdfs (&quot;username&quot;=&quot;hdfs_user&quot;, &quot;password&quot;=&quot;hdfs_password&quot;);

8. Data in load Parquet file specifies FORMAT as parquet. By default, it is judged by file suffix.

    LOAD LABEL example_db.label9
    (
    DATA INFILE(&quot;hdfs://hdfs_host:hdfs_port/user/palo/data/input/file&quot;)
    INTO TABLE `my_table`
    FORMAT AS &quot;parquet&quot;
    (k1, k2, k3)
    )
    WITH BROKER hdfs (&quot;username&quot;=&quot;hdfs_user&quot;, &quot;password&quot;=&quot;hdfs_password&quot;);

9. Extract partition fields in file paths

    If necessary, partitioned fields in the file path are resolved based on the field type defined in the table, similar to the Partition Discovery function in Spark.

    LOAD LABEL example_db.label10
    (
    DATA INFILE(&quot;hdfs://hdfs_host:hdfs_port/user/palo/data/input/dir/city=beijing/*/*&quot;)
    INTO TABLE `my_table`
    FORMAT AS &quot;csv&quot;
    (k1, k2, k3)
    COLUMNS FROM PATH AS (city, utc_date)
    SET (uniq_id = md5sum(k1, city))
    )
    WITH BROKER hdfs (&quot;username&quot;=&quot;hdfs_user&quot;, &quot;password&quot;=&quot;hdfs_password&quot;);

    Directory `hdfs://hdfs_host:hdfs_port/user/palo/data/input/dir/city=beijing` contains following files:
     
    [hdfs://hdfs_host:hdfs_port/user/palo/data/input/dir/city=beijing/utc_date=2019-06-26/0000.csv, hdfs://hdfs_host:hdfs_port/user/palo/data/input/dir/city=beijing/utc_date=2019-06-26/0001.csv, ...]

    Extract city and utc_date fields in the file path

10. To filter the load data, columns whose K1 value is greater than K2 value can be imported.

    LOAD LABEL example_db.label10
    (
    DATA INFILE(&quot;hdfs://hdfs_host:hdfs_port/user/palo/data/input/file&quot;)
    INTO TABLE `my_table`
    where k1 &gt; k2
    );
</pre></div>
</div>
</div>
<div class="section" id="keyword">
<h2>keyword<a class="headerlink" href="#keyword" title="Permalink to this headline">¶</a></h2>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">BROKER</span><span class="p">,</span><span class="n">LOAD</span>
</pre></div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="CANCEL DELETE_EN.html" class="btn btn-neutral float-right" title="CANCEL DELETE" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="index.html" class="btn btn-neutral float-left" title="Data Manipulation" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2019, Apache Doris(Incubating)

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
<div role="contentinfo">
    <p></p>
    <p>

        Apache Doris is an effort undergoing incubation at The Apache Software Foundation (ASF), sponsored by the name of Apache TLP sponsor. Incubation is required of all newly accepted projects until a further review indicates that the infrastructure, communications, and decision making process have stabilized in a manner consistent with other successful ASF projects. While incubation status is not necessarily a reflection of the completeness or stability of the code, it does indicate that the project has yet to be fully endorsed by the ASF.

    </p>
</div>


</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>